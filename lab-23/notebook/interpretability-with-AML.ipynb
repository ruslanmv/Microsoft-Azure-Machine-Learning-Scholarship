{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares And Model Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this quickstart, we will be using a subset of NYC Taxi & Limousine Commission - green taxi trip records available from [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/). The data is enriched with holiday and weather data. We will use data transformations and the GradientBoostingRegressor algorithm from the scikit-learn library to train a regression model to predict taxi fares in New York City based on input features such as, number of passengers, trip distance, datetime, holiday information and weather information.\n",
    "\n",
    "The primary goal of this quickstart is to explain the predictions made by our trained model with the various [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability) packages of the Azure Machine Learning Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "After your install these libraries it is recommended that you **restart** the notebook kernel from the **Kernel** menu above. After restarting the kernel, start from the **Azure Machine Learning and Model Interpretability SDK-specific Imports** section.\n",
    "\n",
    "You can ignore any incompatibility errors. Please run the cell below only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade interpret-community\n",
    "!pip install flask-cors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning and Model Interpretability SDK-specific Imports\n",
    "\n",
    "Remember to restart the kernel before proceeding.\n",
    "\n",
    "Run the following cell to import the modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.externals import joblib\n",
    "import math\n",
    "\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "sklearn_version = sklearn.__version__\n",
    "print('The scikit-learn version is {}.'.format(sklearn_version))\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Experiment, Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "from azureml.interpret.scoring.scoring_explainer import TreeScoringExplainer, save\n",
    "\n",
    "print('The azureml.core version is {}.'.format(azureml.core.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "You can get all of these values from the lab guide on the right:\n",
    "1. In the tabs at the top of the lab guide, select `Environment Details`.\n",
    "2. Copy the values from SubscriptionID, ResourceGroup, WorkspaceName and WorkspaceRegion and paste them as the values in the cell below.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription within the Azure resource group for this lesson\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"\" # <- enter the name of your Azure Resource Group\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"\" # <- enter the name of the Azure Machine Learning workspace\n",
    "workspace_region = \"eastus\" # <- region of your Azure Machine Learning workspace \n",
    "\n",
    "experiment_name = \"lab-explainability\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and connect to an Azure Machine Learning Workspace\n",
    "\n",
    "Run the following cell to connect to your existing Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You may be prompted to login in the text that is output below the cell. If you are, be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to download the dataset, split the data into training and test sets and create a pipeline that includes a few steps to clean and standardize the data and ultimately train the model. \n",
    "\n",
    "NOTE: Do not get too concerned about the details of the following code. If you take anything away from this cell, it should be that a model has been trained and stored in the variable `clf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data_url = ('https://introtomlsampledata.blob.core.windows.net/data/nyc-taxi/nyc-taxi-sample-data.csv')\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    "x_df = df.drop(['totalAmount'], axis=1)\n",
    "y_df = df['totalAmount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=0)\n",
    "\n",
    "categorical = ['normalizeHolidayName', 'isPaidTimeOff']\n",
    "numerical = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', \n",
    "             'day_of_month', 'month_num', 'snowDepth', 'precipTime', 'precipDepth', 'temperature']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])) for f in numerical]\n",
    "    \n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = numeric_transformations + categorical_transformations\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations)),\n",
    "                      ('regressor', GradientBoostingRegressor())])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "y_actual = y_test.values.flatten().tolist()\n",
    "rmse = math.sqrt(mean_squared_error(y_actual, y_predict))\n",
    "print('The RMSE score on test data for GradientBoostingRegressor: ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Explanation Using a Meta Explainer (TabularExplainer)\n",
    "\n",
    "**Global Model Explanation** is a holistic understanding of how the model makes decisions. It provides you with insights on what features are most important and their relative strengths in making model predictions.\n",
    "\n",
    "To initialize an explainer object, you need to pass your model and some training data to the explainer's constructor.\n",
    "\n",
    "*Note that you can pass in your feature transformation pipeline to the explainer to receive explanations in terms of the raw features before the transformation (rather than engineered features).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"features\" and \"classes\" fields are optional\n",
    "trained_gradient_boosting_regressor = clf.steps[-1][1]\n",
    "tabular_explainer = TabularExplainer(trained_gradient_boosting_regressor, \n",
    "                                     initialization_examples=X_train, \n",
    "                                     features=X_train.columns,  \n",
    "                                     transformations=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) uses one of three explainers: TreeExplainer, DeepExplainer, or KernelExplainer, and is automatically selecting the most appropriate one for our use case. \n",
    "\n",
    "You can learn more about the underlying model explainers at [Azure Model Interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the global feature importance values\n",
    "\n",
    "Run the below cell and observe the sorted global feature importance. You will note that `tripDistance` is the most important feature in predicting the taxi fares, followed by `hour_of_day`, and `day_of_week`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the training data or the test data here\n",
    "global_explanation = tabular_explainer.explain_global(X_test)\n",
    "\n",
    "# Sorted feature importance values and feature names\n",
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "dict(zip(sorted_global_importance_names, sorted_global_importance_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Global Explanation\n",
    "\n",
    "Run the following cell to create a dashbaord the enables you to explore the data and visualize the explanation.\n",
    "\n",
    "In the Dashboard that is displayed, try answering the following questions:\n",
    "\n",
    "1. Select the `Data Exploration` tab, the set the `X value` to `tripDistance` and the `Y value` to `PredictedY` (this is predicted fare mount). What happens to the predicted fare as the trip distance increases? \n",
    "2. Select the `Global Importance` tab. Drag the slider under Top K Features so its value is set to `3`. What are the top 3 most important features? Which feature has the highest feature importance (and is therefore the most important feature)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_community.widget import ExplanationDashboard\n",
    "\n",
    "ExplanationDashboard(global_explanation, model=clf, datasetX=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Explanation\n",
    "\n",
    "You can use the [TabularExplainer](https://docs.microsoft.com/en-us/python/api/azureml-explain-model/azureml.explain.model.tabularexplainer?view=azure-ml-py) for a single prediction. You can focus on a single instance and examine model prediction for this input, and explain why.\n",
    "\n",
    "We will create two sample inputs to explain the individual predictions.\n",
    "\n",
    "- **Data 1**\n",
    " - 4 Passengers at 3:00PM, Friday July 5th, temperature 80F, travelling 10 miles\n",
    "\n",
    "- **Data 2**\n",
    " - 1 Passenger at 6:00AM, Monday January 20th, rainy, temperature 35F, travelling 5 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the test dataset\n",
    "columns = ['vendorID', 'passengerCount', 'tripDistance', 'hour_of_day', 'day_of_week', 'day_of_month', \n",
    "           'month_num', 'normalizeHolidayName', 'isPaidTimeOff', 'snowDepth', 'precipTime', \n",
    "           'precipDepth', 'temperature']\n",
    "\n",
    "data = [[1, 4, 10, 15, 4, 5, 7, 'None', False, 0, 0.0, 0.0, 80], \n",
    "        [1, 1, 5, 6, 0, 20, 1, 'Martin Luther King, Jr. Day', True, 0, 2.0, 3.0, 35]]\n",
    "\n",
    "data_df = pd.DataFrame(data, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the test data\n",
    "local_explanation = tabular_explainer.explain_local(data_df)\n",
    "\n",
    "# sorted feature importance values and feature names\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "\n",
    "# package the results in a DataFrame for easy viewing\n",
    "results = pd.DataFrame([sorted_local_importance_names[0][0:5], sorted_local_importance_values[0][0:5], \n",
    "                        sorted_local_importance_names[1][0:5], sorted_local_importance_values[1][0:5]], \n",
    "                       columns = ['1st', '2nd', '3rd', '4th', '5th'], \n",
    "                       index = ['Data 1', '', 'Data 2', ''])\n",
    "print('Top 5 Local Feature Importance')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw from the Global Explanation that the **tripDistance** is the most important global feature. Other than `tripDistance`, the rest of the top 5 important features were different for the two samples.\n",
    "\n",
    "- Data 1: Passenger count 4 and 3:00 PM on Friday were also important features in the prediction.\n",
    "- Data 2: The weather-related features (rainy, temperature 35F), day of the week (Monday) and month (January) were also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're Done!\n",
    "Congratulations you have finished with this lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
